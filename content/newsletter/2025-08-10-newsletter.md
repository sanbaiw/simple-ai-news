---
title: "AI Newsletter - 2025-08-10"
date: "2025-08-10"
draft: false
tags: ["newsletter", "ai"]
categories: ["Daily"]
---

### Models
Claude Opus 4, GPT-4o, GPT-5, Gemini 2.5 Pro, Kimi K2, Qwen3-4B-Instruct, Qwen3-4B-Instruct-2507, Qwen3-4B-Thinking, Qwen3-4B-Thinking-2507
### Companies
Alibaba, Apple, ChinaTalk, DeepSeek, METR, Moonshot, OpenAI, Qwen
### People
David Patterson, Kevin Xu, Sam Altman
### Products/Tools
AI Action Plan, ATOM, Apple silicon, GGUF, Hacker News, LLM, LLMs, LM Studio, MacBook Pro, SVG, WeChat, hn-summary.sh, jq, llm-lmstudio, model picker, tool calling


## Articles

- [What I've been reading (#2): More on Kimi K2, how to build a bad research center, Pretraining with RL, and sporks of AGI](https://www.interconnects.ai/p/what-im-reading-2-more-on-kimi-k2) — The article provides an analysis of the AI landscape, focusing on three areas: how AI companies operate, how people use AI today, and cutting-edge AI tooling. It extensively covers the competitive Chinese AI ecosystem, highlighting Moonshot's Kimi K2 model and its unique venture-backed, B2C business model, contrasting it with DeepSeek and Alibaba. The piece also discusses the intense drive of Chinese AI labs and insights on building effective research centers, referencing David Patterson. Additionally, it delves into the productive use of language models for coding, offering advice on effective LLM-assisted coding and addressing debates on AI's impact on developer productivity.

- [Qwen3-4B-Thinking: "This is art - pelicans don't ride bikes!"](https://simonwillison.net/2025/Aug/10/qwen3-4b/#atom-tag) — The article reviews two new 4B Qwen models, Qwen3-4B-Instruct-2507 and Qwen3-4B-Thinking-2507, noting their surprising capability despite their small size (4GB GGUF). The Qwen3-4B-Thinking model is highlighted for its unique "thinking trace" that enables it to question absurd prompts and display personality. While these models run fast on an M2 MacBook Pro for short tasks, they demonstrate significant slowness when processing longer contexts, such as summarizing a 15,000-token Hacker News conversation. Despite performance limitations on Apple silicon, the author finds them fun and potentially useful for specific problems, particularly noting their effective summarization ability.

- [Quoting Sam Altman](https://simonwillison.net/2025/Aug/10/sam-altman/#atom-tag) — The article reveals a substantial increase in the daily usage of reasoning models, with figures jumping from less than 1% to 7% for free users and from 7% to 24% for plus users. Sam Altman shared these statistics, noting the previously low engagement with the GPT-4o model picker. The text also lists recent article titles discussing LLM updates like GPT-5 and improved tool calling, a model named Qwen3-4B-Thinking, and a talk at an AI Security Meetup.