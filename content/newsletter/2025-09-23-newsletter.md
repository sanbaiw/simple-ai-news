---
title: "AI Newsletter - 2025-09-23"
date: "2025-09-23"
draft: false
tags: ["newsletter", "ai"]
categories: ["Daily"]
---

### Models
Alpaca, ChatGPT, Claude, Claude 4, Evol Instruct, GPT-5, GPT-5 Thinking, Gemma 2, Grok 4, LLMs, Llama 2 8B Instruct, Llama 3 70B Instruct, Llama 3 8B, Llama 3 8B Instruct, Magpie-Air, Magpie-Pro, Nemotron, Qwen-Image-Edit, Qwen-Image-Edit-2509, Qwen3-Next-80B-A3B-Instruct, Qwen3-Next-80B-A3B-Instruct-FP8, Qwen3-Next-80B-A3B-Thinking-FP8, Qwen3-Omni, Qwen3-Omni-30B-A3B-Captioner, Qwen3-Omni-30B-A3B-Instruct, Qwen3-Omni-30B-A3B-Thinking, Qwen3-TTS-Flash, Research Goblin, UltraChat, diffusion language models, o1-preview, o3
### Companies
Apple, GitHub, Google, Hugging Face, Manning, Meta AI, Nvidia, OpenAI, Qwen, xAI
### Products/Tools
API, Chain of Thought (CoT), Code Interpreter, FlashAttention-3, MacBook Air, Ollama, Pyodide, Python, Search (capability), Thinking models, Twitter, chat.qwen.ai, code-execution, coding agents infrastructure, robotics


## Articles

- [Instruction Pretraining LLMs](https://magazine.sebastianraschka.com/p/instruction-pretraining-llms) — The article highlights recent advancements in instruction finetuning for Large Language Models (LLMs), focusing on "Magpie," a novel and cost-effective method for generating high-quality instruction datasets from scratch. This technique automates dataset creation by prompting aligned LLMs like Llama 3 8B, producing diverse data that enables a Llama 3 8B base model to outperform Meta AI's Llama 2 8B Instruct model with significantly fewer training samples. The piece also briefly mentions other significant AI news, including Apple's on-device LLMs, Nvidia's Nemotron, FlashAttention-3, and Google's Gemma 2, and promotes a new educational chapter on instruction finetuning.

- [Thinking, Searching, and Acting](https://www.interconnects.ai/p/thinking-searching-and-acting) — The article details the evolution of language models beyond original ChatGPT weaknesses, focusing on "reasoning models" that integrate three core primitives: Thinking (inference-time scaling through reasoning traces), Searching (accessing non-parametric knowledge stores), and Acting (manipulating physical or digital worlds via tools like code execution and robotics). This paradigm shift allows models to overcome limitations such as hallucinations and static knowledge, fundamentally redefining AI capabilities. Examples include OpenAI's o3 (GPT-5 Thinking), xAI's Grok 4, and Claude 4, which represent different approaches to this new, more enduring form of AI technology, although non-reasoning models retain a role for their efficiency.

- [Four new releases from Qwen](https://simonwillison.net/2025/Sep/22/qwen/#atom-tag) — Team Qwen has announced several new AI model releases and updates. These include FP8 quantized versions of their Qwen3-Next models, the Qwen3-TTS-Flash for multi-lingual and multi-dialect speech synthesis (API only), and the highlight, Qwen3-Omni, a new 30B parameter multimodal model supporting text, audio, and video input with text and audio output, available with open weights. Additionally, an updated version of their Qwen-Image-Edit model, Qwen-Image-Edit-2509, now supports multi-image inputs. The announcements reflect active development in model optimization, multimodal capabilities, and creative AI applications.